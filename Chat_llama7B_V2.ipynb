{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d1e9d66fbed457f8629d64fa1409a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6be0faceae944d72a08bd2a878f49222",
              "IPY_MODEL_e412e604942e46539b5eb9523166fc92",
              "IPY_MODEL_417c9dfc94ca4edabaf3f2bf3435ba33"
            ],
            "layout": "IPY_MODEL_13670af3a450424087229d0bf1914e6d"
          }
        },
        "6be0faceae944d72a08bd2a878f49222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b8dc777739474493225b78f9ca6344",
            "placeholder": "​",
            "style": "IPY_MODEL_5fa29893c6cc4688ba33e504fa97a506",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e412e604942e46539b5eb9523166fc92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eb14e21fd754a4db796660aa0f1f24a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_992bcb74caaf4b66a2a7ad9bd4cf68f7",
            "value": 2
          }
        },
        "417c9dfc94ca4edabaf3f2bf3435ba33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0fa68c82754281b64a6c723a8c6c90",
            "placeholder": "​",
            "style": "IPY_MODEL_658ce3346bc040d5bf96f6877b4953d8",
            "value": " 2/2 [01:08&lt;00:00, 31.22s/it]"
          }
        },
        "13670af3a450424087229d0bf1914e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b8dc777739474493225b78f9ca6344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fa29893c6cc4688ba33e504fa97a506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eb14e21fd754a4db796660aa0f1f24a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "992bcb74caaf4b66a2a7ad9bd4cf68f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b0fa68c82754281b64a6c723a8c6c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "658ce3346bc040d5bf96f6877b4953d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nn75KP-pszRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b87428-444c-426c-bcea-cf4942456c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install the needed packages.\n",
        "!pip install -q -U transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U bitsandbytes einops sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter Efficient Fine Tuning shorten for (peft) is a package that is used in fine tuning the model.\n",
        "# Peft method focuses on training only a subset of the pre-trained model's parameters while freezing the rest of parameters (save gpu Vram).\n",
        "from peft import PeftModel\n",
        "# Import torch which is an open source ML library used for creating deep neural networks.\n",
        "import torch\n",
        "# The transformers library is used to download a pretrained model and a pretrained tokenizer.\n",
        "# Also the BitsAndBytesConfig is the configuration that will allow you to load the model in memory in 4bit or 8bit.\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "9uKEUSsdtPTS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDHMAGxrSigp",
        "outputId": "f3131aea-0258-436a-aeb2-d4c444bef71c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the pretrained model from huggingface\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# the quantization method from bitsandbytes config.\n",
        "config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "\n",
        "# Load the pretrained model from huggingface and apply the quantization method from bitsandbytes config.\n",
        "model = LlamaForCausalLM.from_pretrained(model_id,\n",
        "                                         load_in_4bit=True,\n",
        "                                         torch_dtype=torch.float16,\n",
        "                                         quantization_config=config,\n",
        "                                         device_map=\"auto\")"
      ],
      "metadata": {
        "id": "ftjHPkDTtVxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4d1e9d66fbed457f8629d64fa1409a3f",
            "6be0faceae944d72a08bd2a878f49222",
            "e412e604942e46539b5eb9523166fc92",
            "417c9dfc94ca4edabaf3f2bf3435ba33",
            "13670af3a450424087229d0bf1914e6d",
            "d7b8dc777739474493225b78f9ca6344",
            "5fa29893c6cc4688ba33e504fa97a506",
            "3eb14e21fd754a4db796660aa0f1f24a",
            "992bcb74caaf4b66a2a7ad9bd4cf68f7",
            "3b0fa68c82754281b64a6c723a8c6c90",
            "658ce3346bc040d5bf96f6877b4953d8"
          ]
        },
        "outputId": "e70676e5-196e-4e87-b57a-08a23165ab6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d1e9d66fbed457f8629d64fa1409a3f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iWMibcsag8E",
        "outputId": "30a9ec46-5769-4012-fc9c-597438ffedea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrained tokenizer\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "smrH3yV2tjDx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your question.\n",
        "custom_prompt = \"What are the touristic places i could visit in Mexico?\""
      ],
      "metadata": {
        "id": "21iiu1YytmC6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT =f'''Below is an instruction that describes a task. Write a short response that appropriately completes the request.\n",
        "\n",
        "\n",
        "### Instruction:\n",
        "{custom_prompt}\n",
        "### Response:\n",
        "'''\n",
        "print(\"prompt has been initialised as:\\n\", PROMPT)"
      ],
      "metadata": {
        "id": "cXAqJWk1to0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9857e73-e72f-4f3c-cfeb-0e8efb82e054"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt has been initialised as:\n",
            " Below is an instruction that describes a task. Write a short response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "What are the touristic places i could visit in Mexico?\n",
            "### Response:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Generation function.\n",
        "%%time\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "print(input_ids)\n",
        "print(\"now printing inputs instead:\\n\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "wG_wYW_gtrLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4a0241-0f43-450c-8ea1-eb966eb5bb7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
            "         14350,   263,  3273,  2933,   393,  7128,  2486,  1614,  2167,   278,\n",
            "          2009, 29889,    13,    13,    13,  2277, 29937,  2799,  4080, 29901,\n",
            "            13,  5618,   526,   278,  6282,  4695,  7600,   474,  1033,  6493,\n",
            "           297, 12568, 29973,    13,  2277, 29937, 13291, 29901,    13]],\n",
            "       device='cuda:0')\n",
            "now printing inputs instead:\n",
            "\n",
            "{'input_ids': tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
            "         14350,   263,  3273,  2933,   393,  7128,  2486,  1614,  2167,   278,\n",
            "          2009, 29889,    13,    13,    13,  2277, 29937,  2799,  4080, 29901,\n",
            "            13,  5618,   526,   278,  6282,  4695,  7600,   474,  1033,  6493,\n",
            "           297, 12568, 29973,    13,  2277, 29937, 13291, 29901,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1]])}\n",
            "CPU times: user 5.4 ms, sys: 0 ns, total: 5.4 ms\n",
            "Wall time: 9.83 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    typical_p=1.0,\n",
        "    repetition_penalty=1.0,\n",
        "    encoder_repetition_penalty=1.0,\n",
        "    top_k=40,\n",
        "    # max_length=100\n",
        "    # renormalize_logits=True,\n",
        "    # do_sample=True,\n",
        "    # num_beams=2,\n",
        "    # num_return_sequences=1,\n",
        "    # remove_invalid_values=True\n",
        ")\n",
        "print(\"GENERATION CONFIG INITIALIZED...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dazh8qvlaUIa",
        "outputId": "4ce15705-c810-4de7-f3ce-0a03b68264b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATION CONFIG INITIALIZED...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=False,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "\n",
        "print(generation_output)\n",
        "for s in generation_output.sequences:\n",
        "    print(tokenizer.decode(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FzkMd5ALRb7",
        "outputId": "26ed6735-c661-46cc-b98f-280ba8ba4a17"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GreedySearchDecoderOnlyOutput(sequences=tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
            "         14350,   263,  3273,  2933,   393,  7128,  2486,  1614,  2167,   278,\n",
            "          2009, 29889,    13,    13,    13,  2277, 29937,  2799,  4080, 29901,\n",
            "            13,  5618,   526,   278,  6282,  4695,  7600,   474,  1033,  6493,\n",
            "           297, 12568, 29973,    13,  2277, 29937, 13291, 29901,    13, 29924,\n",
            "           735,  1417,   338,   263,  4234,   411,   263,  8261, 16375,   902,\n",
            "         16639,   322,   263, 16984,  1737,  5275, 29892, 27032,   263,  9377,\n",
            "          3464,   310,  6282,  4695,  7600,   304,  6493, 29889,  3834,   310,\n",
            "           278,  1556,  5972, 15422,   800,  3160,   278, 12297,  2610,   273,\n",
            "          5796,  1144,   310, 27415,   398,   322,   678, 14487,   739,  1362,\n",
            "         29892,   278,   325,  4626,   424,  4272,   310, 12568,  4412,   411,\n",
            "           967, 22879,  4818,   322,  3186, 29899,  1990, 19133, 29879, 29892,\n",
            "           322,   278,  9560,   367, 14520,   310,  1815, 29883,   348,   322,\n",
            "           278, 21505,  8311,  2610, 29874, 29889,  5901,  1818, 29899,  4149,\n",
            "          7600,  3160,   278, 25539,  4272,   310,  3087, 16682,   316,  2178,\n",
            "          3324, 29892,   278,   274,  5989,   310,   278, 15573,  1127, 13939,\n",
            "           310,   278,   530, 15566,  1551,   267, 29892,   322,   278, 11359,\n",
            "           310,   315,  2112,   398,   295, 29889, 26460,   366, 29915,   276,\n",
            "          8852,   297,  4955, 29892,  9257, 29892,  5469, 29892,   470,  3763,\n",
            "         26681,   292,   373,   263,  9560, 25695, 29892, 12568,   756,  1554,\n",
            "           363, 14332, 29889,     2]], device='cuda:0'), scores=None, attentions=None, hidden_states=None)\n",
            "<s> Below is an instruction that describes a task. Write a short response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "What are the touristic places i could visit in Mexico?\n",
            "### Response:\n",
            "Mexico is a country with a rich cultural heritage and a diverse geography, offering a wide range of touristic places to visit. Some of the most popular destinations include the ancient Mayan ruins of Tulum and Chichen Itza, the vibrant city of Mexico City with its historic center and world-class museums, and the beautiful beaches of Cancun and the Riviera Maya. Other must-see places include the colonial city of San Miguel de Allende, the caves of the Sacred Valley of the Ancient Ones, and the island of Cozumel. Whether you're interested in history, culture, nature, or simply relaxing on a beautiful beach, Mexico has something for everyone.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_prompt = \"Out of these attractions that you listed out for Mexico, which are the most peaceful ones, meaning less crowded ones?\""
      ],
      "metadata": {
        "id": "8dHs1DjBcH7s"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT =f'''Below is an instruction that describes a task. Write a short response that appropriately completes the request.\n",
        "\n",
        "\n",
        "### Instruction:\n",
        "{custom_prompt}\n",
        "### Response:\n",
        "'''\n",
        "print(\"prompt has been initialised as:\\n\", PROMPT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX-B2SFbM1yJ",
        "outputId": "be7a8f94-267b-4039-91b9-d6dfb6ee591b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt has been initialised as:\n",
            " Below is an instruction that describes a task. Write a short response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "Out of these attractions that you listed out for Mexico, which are the most peaceful ones, meaning less crowded ones?\n",
            "### Response:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Generation function.\n",
        "%%time\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "print(input_ids)\n",
        "print(\"now printing inputs instead:\\n\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5GFj-6AM56x",
        "outputId": "4fb2b2ec-06d7-4bc2-f6a3-dc7c84865b9b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
            "         14350,   263,  3273,  2933,   393,  7128,  2486,  1614,  2167,   278,\n",
            "          2009, 29889,    13,    13,    13,  2277, 29937,  2799,  4080, 29901,\n",
            "            13,  3744,   310,  1438, 19650,  1953,   393,   366,  9904,   714,\n",
            "           363, 12568, 29892,   607,   526,   278,  1556, 10776,  1319,  6743,\n",
            "         29892,  6593,  3109, 11660,  7176,  6743, 29973,    13,  2277, 29937,\n",
            "         13291, 29901,    13]], device='cuda:0')\n",
            "now printing inputs instead:\n",
            "\n",
            "{'input_ids': tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
            "         14350,   263,  3273,  2933,   393,  7128,  2486,  1614,  2167,   278,\n",
            "          2009, 29889,    13,    13,    13,  2277, 29937,  2799,  4080, 29901,\n",
            "            13,  3744,   310,  1438, 19650,  1953,   393,   366,  9904,   714,\n",
            "           363, 12568, 29892,   607,   526,   278,  1556, 10776,  1319,  6743,\n",
            "         29892,  6593,  3109, 11660,  7176,  6743, 29973,    13,  2277, 29937,\n",
            "         13291, 29901,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "CPU times: user 4.1 ms, sys: 0 ns, total: 4.1 ms\n",
            "Wall time: 4.11 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=False,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "\n",
        "\n",
        "for s in generation_output.sequences:\n",
        "    print(tokenizer.decode(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxIF2upkcVWe",
        "outputId": "64e581c1-55c8-4c25-9414-e5162a4f9413"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> Below is an instruction that describes a task. Write a short response that appropriately completes the request.\n",
            "\n",
            "\n",
            "### Instruction:\n",
            "Out of these attractions that you listed out for Mexico, which are the most peaceful ones, meaning less crowded ones?\n",
            "### Response:\n",
            "Based on my research, the most peaceful and less crowded attractions in Mexico are the ancient Mayan ruins of Tulum and Coba. Both of these sites offer a serene and tranquil atmosphere, with fewer tourists compared to more popular destinations like Cancun and Playa del Carmen. Tulum is located on the Caribbean coast and features a stunning beach and well-preserved ruins, while Coba is a remote jungle site with a vast network of ancient roads and temples. Both of these destinations offer a more authentic and less crowded experience for travelers looking to explore Mexico's rich cultural heritage.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IP2Vlx8OaS9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}